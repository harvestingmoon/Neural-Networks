{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes were extracted from the exerpt by Arjun Sarkar\n",
    "\n",
    "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "\n",
    "This is a notebook which pretty much explains the basics of transformers and what lies underneath the hood. \n",
    "\n",
    "Tbh , you should think of transformers like a cell with many mechanisms inside. \n",
    "\n",
    "The preclusion for this is that you must have at least SOME knowledge on the basics of neural networks because this notebook wouldn't cover everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi - Head Attention Mechanism\n",
    "\n",
    "In short, the multi-head attention mechanism is the heart of the entire transformer model.\n",
    "\n",
    "By Copilot:\n",
    "\n",
    "Multi-head attention is a mechanism in Transformer models that allows the model to focus on different parts of the input sequence simultaneously. It improves the model's ability to capture various aspects of the data by running multiple attention mechanisms in parallel. Each \"head\" in this context refers to a separate attention mechanism, with each focusing on different parts of the input to gather diverse information. The outputs of these heads are then combined and processed further. This approach enhances the model's ability to understand complex relationships in the data, leading to better performance on tasks like translation, text summarization, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert d_model % heads == 0, \"d model must be divisible by num heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Values\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k) # Attention score is calculated before being reshaped to 2 x 1 matrix \n",
    "        if mask  is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim = -1) \n",
    "        output = torch.matmul(attn_probs, V) # \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def split_heads(self,x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.heads, self.d_k).transpose(1,2)\n",
    "\n",
    "    def combine_heads(self,x): # Area where the output is reshaped and combines output from all the heads\n",
    "        print(x.size())\n",
    "        batch_size,_, seq_length,d_k = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask = None): # => allows model to focus on some aspets of i/p sequencing\n",
    "        \n",
    "        Q = self.split_heads(self.W_q(Q)) #Splits the heads\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q,K,V, mask) # Attention score is calculated \n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalFF(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionalFF, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding (For injecting position information of each token into input seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder : Multi-Head Attention => Position-wise FFN => 2 Layer Normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionalFF(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x,x,x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder: Self Attention => Cross Attention => FF => Linear => SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionalFF(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x,x,x, tgt_mask) # Calculates the self attention scores\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # Normalisation\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) #X taken from attn_output 1 & Encoder Output 1 taken from Encoder => producing the output\n",
    "        x = self.norm2(x + self.dropout(attn_output)) #Normalisation\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt): # Creates binary mask for src & Target seq to ignore padding tokens => preventing decoder from attending to future tokens\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 1 , Loss : 8.694376945495605\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 2 , Loss : 8.558024406433105\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 3 , Loss : 8.483941078186035\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 4 , Loss : 8.429970741271973\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 5 , Loss : 8.37327766418457\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 6 , Loss : 8.300586700439453\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 7 , Loss : 8.22421646118164\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 8 , Loss : 8.135527610778809\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 9 , Loss : 8.057876586914062\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 10 , Loss : 7.969840049743652\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 11 , Loss : 7.9005632400512695\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 12 , Loss : 7.8146233558654785\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 13 , Loss : 7.726254463195801\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 14 , Loss : 7.644623279571533\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 15 , Loss : 7.564270496368408\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 16 , Loss : 7.478582382202148\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 17 , Loss : 7.391839504241943\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 18 , Loss : 7.310111045837402\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 19 , Loss : 7.2270965576171875\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 20 , Loss : 7.1554059982299805\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 21 , Loss : 7.07037353515625\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 22 , Loss : 6.991754055023193\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 23 , Loss : 6.923823833465576\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 24 , Loss : 6.835569858551025\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 25 , Loss : 6.761805057525635\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 26 , Loss : 6.6866278648376465\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 27 , Loss : 6.610222816467285\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 28 , Loss : 6.5515265464782715\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 29 , Loss : 6.485195636749268\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 30 , Loss : 6.407726764678955\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 31 , Loss : 6.3319478034973145\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 32 , Loss : 6.261379241943359\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 33 , Loss : 6.19972562789917\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 34 , Loss : 6.125980854034424\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 35 , Loss : 6.055135250091553\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 36 , Loss : 5.996394634246826\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 37 , Loss : 5.922679424285889\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 38 , Loss : 5.861860275268555\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 39 , Loss : 5.804293632507324\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 40 , Loss : 5.7448039054870605\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 41 , Loss : 5.679135322570801\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 42 , Loss : 5.618601322174072\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 43 , Loss : 5.561527729034424\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 44 , Loss : 5.497764587402344\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 45 , Loss : 5.436281681060791\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 46 , Loss : 5.377297878265381\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 47 , Loss : 5.309500694274902\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 48 , Loss : 5.258561134338379\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 49 , Loss : 5.19828462600708\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 50 , Loss : 5.1402411460876465\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 51 , Loss : 5.084436416625977\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 52 , Loss : 5.0279011726379395\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 53 , Loss : 4.970423698425293\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "Epoch: 54 , Loss : 4.921064376831055\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 100, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n",
      "torch.Size([64, 8, 99, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(src_data, tgt_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , Loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\hack\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\hack\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size,tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index =0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr = 0.0001, betas = (0.9,0.98), eps = 1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch + 1} , Loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing The Model... I forgot to test-train-split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[7.6205e-01, 9.9327e-01, 5.0873e-01,  ..., 7.6584e-01,\n",
      "           7.2699e-03, 6.6235e-01],\n",
      "          [1.7281e-01, 6.1966e-01, 4.8188e-01,  ..., 6.8336e-01,\n",
      "           2.1861e-01, 5.6384e-01],\n",
      "          [3.7274e-01, 7.9061e-01, 9.2151e-01,  ..., 4.0698e-01,\n",
      "           1.3234e-01, 3.4084e-01],\n",
      "          ...,\n",
      "          [7.7362e-01, 1.3355e-02, 8.7557e-01,  ..., 1.1168e-01,\n",
      "           6.1797e-01, 5.5582e-01],\n",
      "          [9.2714e-01, 1.6533e-01, 1.5321e-01,  ..., 2.7326e-01,\n",
      "           3.5379e-01, 3.8937e-01],\n",
      "          [5.9837e-01, 1.7890e-01, 1.7711e-01,  ..., 2.5047e-01,\n",
      "           7.7111e-01, 9.6134e-01]],\n",
      "\n",
      "         [[1.0337e-01, 2.9498e-02, 7.0527e-01,  ..., 5.9516e-01,\n",
      "           1.1848e-01, 8.2698e-01],\n",
      "          [9.2683e-01, 1.6115e-01, 6.4840e-01,  ..., 9.4216e-01,\n",
      "           6.1256e-01, 4.0105e-01],\n",
      "          [1.3863e-01, 8.3464e-01, 6.8435e-01,  ..., 7.5664e-01,\n",
      "           8.2617e-01, 6.5980e-01],\n",
      "          ...,\n",
      "          [3.8531e-01, 4.0938e-02, 3.9644e-01,  ..., 3.7407e-01,\n",
      "           9.6149e-01, 1.4958e-01],\n",
      "          [2.7553e-01, 2.8381e-01, 1.2398e-01,  ..., 7.2112e-01,\n",
      "           5.2542e-01, 7.9506e-01],\n",
      "          [1.5826e-01, 3.4057e-01, 7.6745e-01,  ..., 9.1339e-01,\n",
      "           5.2247e-01, 4.5443e-01]],\n",
      "\n",
      "         [[8.3210e-01, 3.7059e-01, 2.9896e-01,  ..., 3.1045e-01,\n",
      "           1.9914e-01, 6.1145e-01],\n",
      "          [6.1026e-01, 2.1091e-01, 3.6303e-01,  ..., 4.7549e-01,\n",
      "           1.2574e-01, 6.7155e-01],\n",
      "          [4.7772e-01, 4.9900e-01, 7.8506e-01,  ..., 9.6210e-01,\n",
      "           1.9414e-01, 9.9204e-01],\n",
      "          ...,\n",
      "          [7.8697e-01, 8.3291e-01, 5.7640e-01,  ..., 5.2608e-01,\n",
      "           4.6018e-01, 9.4301e-01],\n",
      "          [7.8308e-01, 1.4128e-01, 4.8058e-01,  ..., 1.1677e-01,\n",
      "           1.7592e-01, 5.3947e-01],\n",
      "          [5.1542e-01, 2.9051e-01, 6.0016e-01,  ..., 6.3718e-01,\n",
      "           3.7184e-01, 5.4184e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[6.4248e-01, 1.3224e-01, 8.7407e-01,  ..., 7.4947e-01,\n",
      "           6.6986e-01, 7.9601e-01],\n",
      "          [7.5018e-01, 1.1991e-01, 5.5713e-01,  ..., 4.2025e-01,\n",
      "           6.9862e-01, 4.6445e-01],\n",
      "          [2.9463e-01, 5.6479e-01, 3.1170e-01,  ..., 4.5230e-01,\n",
      "           8.0037e-01, 2.1073e-01],\n",
      "          ...,\n",
      "          [8.6424e-01, 2.0889e-01, 7.0669e-02,  ..., 8.3249e-01,\n",
      "           8.1028e-01, 3.4430e-01],\n",
      "          [5.6164e-01, 2.1045e-01, 2.4080e-01,  ..., 7.1085e-01,\n",
      "           1.0802e-01, 2.2256e-01],\n",
      "          [9.1841e-02, 7.6477e-01, 7.8939e-01,  ..., 3.7160e-02,\n",
      "           9.0577e-01, 8.9928e-01]],\n",
      "\n",
      "         [[1.7604e-01, 3.2942e-01, 4.3670e-01,  ..., 8.7191e-01,\n",
      "           4.7032e-01, 5.4648e-01],\n",
      "          [6.7131e-01, 9.2087e-01, 4.8184e-01,  ..., 5.1532e-01,\n",
      "           8.2977e-01, 2.7541e-01],\n",
      "          [4.0636e-01, 4.3387e-02, 9.8000e-01,  ..., 6.7889e-01,\n",
      "           1.5408e-01, 3.0139e-01],\n",
      "          ...,\n",
      "          [8.7726e-01, 7.7023e-01, 6.0162e-01,  ..., 1.4819e-01,\n",
      "           8.4976e-01, 6.2684e-01],\n",
      "          [9.9636e-01, 1.1237e-01, 3.4562e-02,  ..., 6.1034e-01,\n",
      "           6.8688e-01, 8.8453e-01],\n",
      "          [4.1046e-01, 3.6706e-02, 2.4023e-01,  ..., 7.7674e-01,\n",
      "           3.5206e-01, 8.5330e-01]],\n",
      "\n",
      "         [[7.0373e-01, 7.3327e-02, 1.9111e-01,  ..., 2.8935e-01,\n",
      "           9.6047e-01, 4.6824e-01],\n",
      "          [2.0619e-02, 2.3898e-01, 6.8966e-01,  ..., 3.9024e-01,\n",
      "           5.1313e-01, 5.5860e-02],\n",
      "          [7.4511e-02, 2.8498e-01, 6.0877e-01,  ..., 4.9299e-01,\n",
      "           9.4658e-01, 9.7588e-01],\n",
      "          ...,\n",
      "          [8.0683e-01, 9.9906e-01, 2.9562e-01,  ..., 2.6328e-01,\n",
      "           5.1711e-01, 7.8862e-01],\n",
      "          [9.4436e-01, 3.0407e-01, 6.9198e-01,  ..., 9.4599e-01,\n",
      "           5.8723e-01, 1.3725e-02],\n",
      "          [7.8599e-01, 3.3730e-01, 2.4450e-01,  ..., 6.3571e-01,\n",
      "           8.7323e-01, 4.1165e-02]]],\n",
      "\n",
      "\n",
      "        [[[9.1712e-01, 7.5158e-01, 6.7795e-01,  ..., 2.5890e-02,\n",
      "           2.9573e-01, 6.6300e-01],\n",
      "          [8.8407e-01, 4.6477e-01, 1.9347e-01,  ..., 4.9227e-01,\n",
      "           4.4952e-01, 4.1993e-01],\n",
      "          [8.8370e-01, 7.9105e-02, 7.4018e-01,  ..., 4.4177e-01,\n",
      "           5.6213e-01, 2.6815e-01],\n",
      "          ...,\n",
      "          [2.2765e-01, 6.5741e-01, 8.5236e-01,  ..., 2.3687e-01,\n",
      "           6.4811e-01, 5.4064e-01],\n",
      "          [5.8245e-01, 8.4842e-01, 8.6871e-01,  ..., 6.7536e-01,\n",
      "           2.2174e-01, 4.4240e-01],\n",
      "          [9.0158e-01, 6.5231e-01, 7.5220e-01,  ..., 4.1935e-01,\n",
      "           1.0394e-01, 5.6144e-01]],\n",
      "\n",
      "         [[9.9609e-01, 7.8799e-01, 9.0858e-01,  ..., 8.8829e-01,\n",
      "           4.4504e-01, 9.1505e-01],\n",
      "          [4.0224e-01, 2.6085e-01, 6.4844e-01,  ..., 1.5831e-01,\n",
      "           7.1439e-01, 2.0836e-01],\n",
      "          [8.7898e-01, 2.4653e-01, 8.7039e-01,  ..., 1.9062e-02,\n",
      "           9.4278e-01, 6.1348e-01],\n",
      "          ...,\n",
      "          [8.9388e-01, 8.2347e-01, 1.8630e-01,  ..., 6.0879e-01,\n",
      "           1.6197e-01, 4.3536e-01],\n",
      "          [3.4282e-02, 9.1378e-01, 1.2881e-01,  ..., 3.7332e-01,\n",
      "           7.3776e-01, 1.2187e-01],\n",
      "          [4.4804e-01, 8.2800e-01, 5.7371e-01,  ..., 2.9876e-01,\n",
      "           5.9297e-01, 8.2085e-01]],\n",
      "\n",
      "         [[6.5214e-01, 9.9078e-01, 8.5450e-01,  ..., 7.0929e-01,\n",
      "           6.8622e-01, 7.0978e-01],\n",
      "          [3.2083e-01, 8.1855e-01, 4.4312e-01,  ..., 2.2049e-01,\n",
      "           4.4233e-01, 9.3904e-01],\n",
      "          [5.0024e-01, 2.2917e-01, 1.3967e-01,  ..., 9.9755e-01,\n",
      "           8.1190e-01, 6.2338e-01],\n",
      "          ...,\n",
      "          [4.2246e-01, 8.6937e-01, 3.3051e-01,  ..., 9.2256e-01,\n",
      "           3.3889e-01, 4.3236e-01],\n",
      "          [9.6108e-01, 3.0042e-01, 2.1756e-02,  ..., 3.1128e-01,\n",
      "           2.5735e-01, 1.1042e-02],\n",
      "          [2.6454e-01, 4.9977e-01, 7.7828e-01,  ..., 3.7218e-01,\n",
      "           6.7353e-01, 5.9886e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.4018e-02, 1.1236e-01, 6.6239e-01,  ..., 2.6533e-01,\n",
      "           7.5237e-01, 7.8198e-01],\n",
      "          [6.5062e-01, 3.3711e-01, 8.6066e-01,  ..., 2.7624e-01,\n",
      "           8.3921e-01, 9.8403e-01],\n",
      "          [4.9168e-01, 1.1792e-01, 8.6182e-01,  ..., 6.5774e-01,\n",
      "           8.7885e-01, 2.1460e-01],\n",
      "          ...,\n",
      "          [1.6643e-01, 8.6824e-02, 6.9372e-01,  ..., 4.3639e-01,\n",
      "           7.7762e-01, 8.4306e-01],\n",
      "          [7.2244e-01, 3.2601e-01, 8.3478e-01,  ..., 4.7669e-01,\n",
      "           4.3334e-01, 3.7511e-01],\n",
      "          [5.1898e-02, 1.1434e-01, 8.8923e-02,  ..., 5.9528e-01,\n",
      "           4.9056e-01, 5.4186e-01]],\n",
      "\n",
      "         [[3.8823e-01, 9.6198e-01, 9.1471e-01,  ..., 1.7264e-01,\n",
      "           5.1295e-01, 9.3255e-01],\n",
      "          [5.0148e-01, 3.3139e-01, 6.4722e-01,  ..., 7.9145e-01,\n",
      "           6.3004e-01, 7.6776e-01],\n",
      "          [3.6541e-01, 8.6086e-01, 3.3201e-01,  ..., 1.8593e-01,\n",
      "           2.1063e-01, 3.6918e-01],\n",
      "          ...,\n",
      "          [6.3579e-01, 3.0020e-01, 7.8129e-01,  ..., 3.8858e-01,\n",
      "           5.9499e-02, 9.8843e-01],\n",
      "          [5.4867e-01, 2.7495e-01, 1.7390e-01,  ..., 2.8727e-01,\n",
      "           8.1655e-01, 8.2896e-01],\n",
      "          [9.0917e-01, 5.3593e-01, 7.1344e-02,  ..., 4.7022e-02,\n",
      "           4.0822e-01, 1.8263e-01]],\n",
      "\n",
      "         [[7.0867e-01, 2.0159e-03, 9.1728e-01,  ..., 7.7966e-01,\n",
      "           1.6633e-03, 1.9675e-01],\n",
      "          [9.5797e-01, 8.3983e-05, 7.7264e-01,  ..., 5.8253e-01,\n",
      "           7.7233e-01, 6.8670e-01],\n",
      "          [5.7644e-01, 7.9259e-01, 2.0053e-02,  ..., 9.0675e-01,\n",
      "           5.9764e-03, 8.4357e-01],\n",
      "          ...,\n",
      "          [6.0206e-02, 6.5647e-01, 5.6897e-01,  ..., 5.4060e-02,\n",
      "           1.0870e-01, 9.6508e-01],\n",
      "          [1.6721e-01, 7.3938e-01, 6.0374e-01,  ..., 9.9455e-01,\n",
      "           7.8956e-01, 3.5993e-01],\n",
      "          [4.8027e-01, 1.4789e-01, 4.6464e-01,  ..., 6.3193e-01,\n",
      "           1.3881e-01, 3.7801e-01]]],\n",
      "\n",
      "\n",
      "        [[[9.9519e-01, 5.9553e-02, 7.2107e-01,  ..., 1.2541e-01,\n",
      "           7.9893e-01, 3.9689e-01],\n",
      "          [5.7578e-01, 3.6826e-01, 1.6218e-01,  ..., 2.8632e-01,\n",
      "           9.6674e-01, 4.6851e-01],\n",
      "          [3.7850e-03, 2.4866e-01, 7.0569e-01,  ..., 2.6716e-01,\n",
      "           9.8042e-01, 7.8710e-01],\n",
      "          ...,\n",
      "          [2.7635e-02, 2.6260e-01, 2.9567e-01,  ..., 5.9237e-01,\n",
      "           7.1002e-01, 9.3293e-01],\n",
      "          [5.3908e-01, 6.0122e-01, 2.7475e-01,  ..., 6.2950e-01,\n",
      "           8.7499e-01, 1.1110e-01],\n",
      "          [7.9504e-02, 4.6845e-01, 5.3128e-01,  ..., 3.2455e-01,\n",
      "           2.5332e-01, 8.1082e-01]],\n",
      "\n",
      "         [[2.3180e-01, 7.0523e-02, 1.4492e-02,  ..., 3.9842e-01,\n",
      "           5.7026e-01, 5.6044e-01],\n",
      "          [8.2189e-02, 1.9417e-01, 1.5908e-01,  ..., 1.6160e-01,\n",
      "           9.8525e-01, 4.0107e-01],\n",
      "          [2.0339e-01, 6.6056e-01, 3.2327e-01,  ..., 1.9351e-01,\n",
      "           2.0254e-01, 1.9503e-01],\n",
      "          ...,\n",
      "          [3.8281e-01, 2.1535e-01, 6.6595e-01,  ..., 9.7113e-01,\n",
      "           3.9189e-01, 3.6548e-01],\n",
      "          [8.6839e-02, 8.9428e-01, 8.1369e-01,  ..., 8.1369e-01,\n",
      "           8.6084e-01, 1.8208e-01],\n",
      "          [3.9796e-01, 1.1683e-01, 8.9379e-01,  ..., 6.6520e-01,\n",
      "           3.2149e-01, 2.8686e-02]],\n",
      "\n",
      "         [[9.8939e-02, 9.1442e-01, 4.9177e-01,  ..., 7.9334e-02,\n",
      "           7.0929e-01, 6.3670e-01],\n",
      "          [3.3210e-01, 4.7248e-01, 6.3590e-01,  ..., 9.3286e-01,\n",
      "           3.2674e-01, 6.6086e-01],\n",
      "          [5.9717e-01, 6.3373e-01, 4.2562e-03,  ..., 9.0488e-01,\n",
      "           3.5622e-01, 3.0796e-01],\n",
      "          ...,\n",
      "          [4.5895e-01, 4.3339e-01, 8.9191e-02,  ..., 1.7196e-01,\n",
      "           8.4606e-01, 9.8175e-01],\n",
      "          [3.4379e-01, 3.5432e-01, 6.3433e-01,  ..., 8.5482e-01,\n",
      "           9.9330e-01, 2.3332e-01],\n",
      "          [5.6025e-01, 9.9562e-01, 9.5352e-01,  ..., 7.8289e-01,\n",
      "           9.1951e-01, 5.8342e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.7112e-01, 4.9131e-01, 7.4157e-01,  ..., 9.2117e-01,\n",
      "           2.3597e-01, 5.9554e-01],\n",
      "          [9.2893e-01, 5.0738e-01, 4.9763e-01,  ..., 9.0063e-02,\n",
      "           9.0463e-01, 8.9773e-01],\n",
      "          [4.0428e-01, 7.8578e-01, 7.2693e-01,  ..., 8.2676e-01,\n",
      "           5.0103e-01, 6.6728e-01],\n",
      "          ...,\n",
      "          [2.3310e-01, 2.1516e-01, 4.0291e-01,  ..., 8.2713e-01,\n",
      "           4.4960e-01, 9.0964e-01],\n",
      "          [4.1994e-01, 4.0270e-01, 3.6008e-01,  ..., 9.2126e-01,\n",
      "           5.2698e-01, 7.1452e-01],\n",
      "          [3.5232e-01, 1.6951e-01, 7.1193e-01,  ..., 7.0125e-01,\n",
      "           4.1202e-01, 7.2716e-01]],\n",
      "\n",
      "         [[7.1221e-01, 8.9467e-01, 1.5690e-01,  ..., 3.1948e-01,\n",
      "           4.5054e-01, 2.9490e-01],\n",
      "          [4.4816e-01, 5.1643e-01, 7.7095e-01,  ..., 1.7554e-01,\n",
      "           4.8597e-01, 4.8155e-01],\n",
      "          [6.3464e-01, 9.3244e-01, 9.1906e-01,  ..., 1.3126e-01,\n",
      "           9.2082e-01, 8.7151e-01],\n",
      "          ...,\n",
      "          [3.7816e-01, 5.5304e-01, 6.0138e-01,  ..., 5.3619e-01,\n",
      "           3.2878e-02, 3.6717e-01],\n",
      "          [2.7351e-01, 5.9685e-01, 5.5189e-01,  ..., 3.0918e-01,\n",
      "           9.5328e-01, 4.4510e-01],\n",
      "          [5.7350e-01, 8.9744e-01, 7.0233e-01,  ..., 2.8028e-01,\n",
      "           9.0781e-01, 5.7280e-01]],\n",
      "\n",
      "         [[6.3090e-02, 1.9119e-01, 9.2832e-01,  ..., 3.3626e-01,\n",
      "           1.6700e-01, 5.3417e-01],\n",
      "          [9.2609e-01, 1.9496e-01, 7.2127e-01,  ..., 2.3304e-01,\n",
      "           6.7035e-01, 9.3834e-01],\n",
      "          [8.0251e-01, 2.3445e-01, 6.3448e-01,  ..., 5.2574e-01,\n",
      "           6.4144e-01, 6.0389e-01],\n",
      "          ...,\n",
      "          [7.4666e-01, 6.2543e-01, 2.8174e-01,  ..., 1.7591e-01,\n",
      "           9.1375e-02, 9.9645e-03],\n",
      "          [2.2200e-01, 8.8504e-01, 7.5443e-01,  ..., 6.8191e-01,\n",
      "           6.4359e-01, 1.6452e-01],\n",
      "          [2.6108e-01, 6.7884e-01, 7.4286e-03,  ..., 8.0469e-01,\n",
      "           6.8908e-01, 7.6112e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[5.4265e-01, 2.6840e-01, 3.4243e-01,  ..., 9.4310e-02,\n",
      "           9.5418e-01, 7.1417e-01],\n",
      "          [5.3548e-02, 5.7282e-01, 5.3812e-01,  ..., 2.4078e-01,\n",
      "           7.1953e-01, 1.8065e-01],\n",
      "          [3.3916e-01, 6.2899e-01, 1.6827e-02,  ..., 4.7970e-01,\n",
      "           1.5952e-02, 2.4132e-01],\n",
      "          ...,\n",
      "          [4.4562e-01, 1.8080e-01, 4.7406e-01,  ..., 2.9462e-01,\n",
      "           9.2639e-01, 7.3775e-01],\n",
      "          [8.3011e-01, 5.1756e-01, 2.9081e-01,  ..., 8.4295e-01,\n",
      "           7.3939e-02, 4.4076e-01],\n",
      "          [2.6437e-01, 3.6516e-01, 2.9136e-02,  ..., 9.3735e-01,\n",
      "           3.2345e-01, 7.5792e-01]],\n",
      "\n",
      "         [[6.9993e-01, 8.9118e-01, 3.1093e-01,  ..., 5.5928e-01,\n",
      "           5.4318e-01, 8.9242e-02],\n",
      "          [3.8086e-01, 9.7402e-01, 3.0788e-01,  ..., 6.0364e-01,\n",
      "           9.0632e-01, 2.7363e-01],\n",
      "          [6.1753e-01, 9.9622e-01, 3.3952e-01,  ..., 6.9992e-01,\n",
      "           1.8008e-01, 1.1199e-01],\n",
      "          ...,\n",
      "          [8.6431e-01, 9.1084e-01, 6.5894e-01,  ..., 7.3398e-01,\n",
      "           8.5142e-01, 3.1021e-01],\n",
      "          [9.6117e-01, 8.8071e-01, 9.7938e-01,  ..., 5.2062e-01,\n",
      "           3.3399e-01, 1.4983e-01],\n",
      "          [8.4892e-01, 9.5403e-01, 1.7330e-01,  ..., 6.2966e-01,\n",
      "           5.4546e-01, 1.9618e-01]],\n",
      "\n",
      "         [[4.8850e-01, 6.1133e-01, 1.1491e-01,  ..., 4.9462e-01,\n",
      "           1.5930e-01, 5.1127e-01],\n",
      "          [2.3190e-02, 2.4764e-01, 8.2429e-01,  ..., 7.3135e-01,\n",
      "           4.8090e-01, 6.2657e-01],\n",
      "          [3.0276e-01, 8.3107e-01, 9.0700e-02,  ..., 8.8294e-01,\n",
      "           2.8208e-01, 5.9319e-01],\n",
      "          ...,\n",
      "          [5.1081e-01, 9.6658e-02, 2.6790e-02,  ..., 7.0991e-01,\n",
      "           7.5768e-01, 7.3568e-01],\n",
      "          [1.6290e-01, 1.6254e-01, 7.7334e-01,  ..., 2.2324e-01,\n",
      "           6.3188e-01, 9.9323e-01],\n",
      "          [3.9812e-01, 4.5780e-01, 5.2758e-01,  ..., 4.0627e-01,\n",
      "           3.9166e-01, 7.0483e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.0200e-02, 8.4882e-01, 6.8361e-02,  ..., 6.5235e-01,\n",
      "           1.7183e-01, 9.3833e-01],\n",
      "          [7.8783e-01, 4.6031e-01, 4.0200e-02,  ..., 8.2195e-01,\n",
      "           2.6969e-01, 2.4667e-03],\n",
      "          [4.9009e-01, 9.2524e-01, 2.8744e-01,  ..., 1.8061e-01,\n",
      "           2.9642e-01, 5.1606e-01],\n",
      "          ...,\n",
      "          [1.0009e-01, 8.8910e-01, 4.0966e-01,  ..., 3.5111e-01,\n",
      "           6.9444e-01, 3.8322e-01],\n",
      "          [9.5581e-01, 9.0320e-01, 4.4657e-01,  ..., 1.0989e-01,\n",
      "           3.6672e-01, 3.4767e-01],\n",
      "          [3.7471e-01, 1.1927e-01, 8.8752e-01,  ..., 7.9153e-01,\n",
      "           5.3364e-01, 4.2405e-01]],\n",
      "\n",
      "         [[2.6654e-01, 6.5155e-01, 2.9306e-01,  ..., 7.4882e-01,\n",
      "           7.2191e-01, 7.4088e-01],\n",
      "          [4.4287e-01, 4.3775e-01, 4.5920e-01,  ..., 2.3452e-01,\n",
      "           3.6613e-01, 1.6217e-01],\n",
      "          [3.9757e-01, 3.5794e-01, 3.3997e-01,  ..., 3.4737e-01,\n",
      "           1.5676e-01, 5.3362e-02],\n",
      "          ...,\n",
      "          [8.1396e-01, 9.8785e-01, 5.4231e-02,  ..., 3.0670e-01,\n",
      "           7.3875e-01, 8.6254e-01],\n",
      "          [2.8950e-01, 6.2221e-01, 5.7850e-01,  ..., 3.9648e-01,\n",
      "           4.0330e-01, 2.7852e-01],\n",
      "          [2.9587e-01, 2.2813e-01, 7.6329e-01,  ..., 5.9540e-01,\n",
      "           4.2236e-01, 3.2966e-01]],\n",
      "\n",
      "         [[4.8262e-01, 1.0544e-01, 9.6699e-01,  ..., 8.5617e-02,\n",
      "           9.3694e-01, 7.5645e-01],\n",
      "          [3.2934e-01, 2.6221e-01, 8.6440e-01,  ..., 3.9108e-01,\n",
      "           8.5728e-01, 6.3660e-01],\n",
      "          [9.1873e-01, 3.9690e-01, 5.3003e-01,  ..., 7.3934e-01,\n",
      "           9.2503e-01, 1.8461e-01],\n",
      "          ...,\n",
      "          [5.6077e-01, 1.0985e-01, 8.0915e-01,  ..., 3.1501e-01,\n",
      "           7.8745e-01, 3.1635e-01],\n",
      "          [3.0762e-02, 5.3100e-01, 1.9444e-01,  ..., 9.7762e-01,\n",
      "           1.3518e-01, 2.7275e-01],\n",
      "          [5.9947e-01, 9.9624e-01, 2.3006e-01,  ..., 4.8667e-01,\n",
      "           9.4461e-01, 3.1237e-01]]],\n",
      "\n",
      "\n",
      "        [[[9.5260e-01, 1.3740e-01, 6.8045e-01,  ..., 8.0291e-01,\n",
      "           1.7596e-01, 6.3936e-01],\n",
      "          [4.9651e-01, 8.4081e-01, 9.1360e-02,  ..., 5.0903e-01,\n",
      "           5.5779e-01, 4.0142e-01],\n",
      "          [3.9867e-01, 9.1633e-01, 9.1954e-01,  ..., 7.7512e-01,\n",
      "           7.4147e-03, 6.1322e-01],\n",
      "          ...,\n",
      "          [9.1653e-01, 8.1349e-01, 6.8038e-01,  ..., 2.4093e-01,\n",
      "           8.4834e-01, 2.8675e-01],\n",
      "          [1.1557e-01, 1.3527e-01, 8.0297e-02,  ..., 1.8857e-01,\n",
      "           2.3622e-01, 1.7044e-01],\n",
      "          [6.3485e-01, 2.5509e-01, 8.0840e-01,  ..., 9.9664e-01,\n",
      "           7.6433e-01, 7.6023e-01]],\n",
      "\n",
      "         [[6.5370e-01, 5.8111e-01, 2.1688e-01,  ..., 9.0871e-01,\n",
      "           7.5215e-01, 5.8003e-01],\n",
      "          [9.4368e-01, 6.6161e-01, 1.5706e-01,  ..., 1.3959e-01,\n",
      "           6.5917e-01, 4.9364e-01],\n",
      "          [7.4451e-01, 7.9460e-01, 3.9601e-01,  ..., 3.6895e-01,\n",
      "           6.1083e-01, 5.2342e-01],\n",
      "          ...,\n",
      "          [3.3575e-02, 7.6524e-01, 5.1286e-01,  ..., 7.6494e-01,\n",
      "           9.5779e-01, 6.5983e-01],\n",
      "          [5.9888e-01, 7.7651e-01, 9.3406e-01,  ..., 6.4486e-01,\n",
      "           2.5477e-01, 5.8166e-01],\n",
      "          [4.1342e-01, 3.6118e-01, 6.7995e-01,  ..., 5.5046e-01,\n",
      "           5.2402e-03, 3.3367e-01]],\n",
      "\n",
      "         [[4.6777e-01, 6.0811e-01, 6.8294e-01,  ..., 1.1454e-01,\n",
      "           1.3249e-01, 1.7935e-01],\n",
      "          [3.8212e-01, 2.8304e-01, 8.9432e-02,  ..., 4.1670e-01,\n",
      "           1.6310e-01, 4.4684e-01],\n",
      "          [3.2696e-01, 7.3369e-01, 3.1320e-01,  ..., 8.8585e-01,\n",
      "           9.1222e-01, 4.6789e-01],\n",
      "          ...,\n",
      "          [4.9420e-01, 4.7061e-01, 9.6081e-01,  ..., 1.4684e-01,\n",
      "           4.1134e-01, 1.1023e-01],\n",
      "          [1.4674e-01, 9.5498e-01, 7.2411e-02,  ..., 4.0305e-01,\n",
      "           5.2172e-01, 1.2352e-01],\n",
      "          [8.7335e-01, 1.6562e-01, 4.6742e-01,  ..., 1.1662e-01,\n",
      "           9.1963e-01, 4.3737e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.4214e-01, 3.9921e-01, 3.2774e-01,  ..., 3.1601e-01,\n",
      "           7.7231e-01, 8.2342e-01],\n",
      "          [7.6096e-01, 9.1354e-01, 8.2140e-01,  ..., 5.8868e-01,\n",
      "           3.8189e-01, 2.0197e-01],\n",
      "          [7.3325e-01, 9.7404e-01, 8.1933e-01,  ..., 6.7913e-01,\n",
      "           9.2239e-01, 3.6804e-02],\n",
      "          ...,\n",
      "          [2.5253e-01, 6.2536e-01, 5.5471e-01,  ..., 6.4145e-01,\n",
      "           7.7932e-01, 3.4522e-01],\n",
      "          [3.3317e-01, 4.5779e-01, 3.1230e-01,  ..., 7.2564e-01,\n",
      "           4.8831e-01, 2.4169e-01],\n",
      "          [5.3939e-01, 4.7963e-01, 8.1148e-01,  ..., 3.4225e-01,\n",
      "           6.3432e-02, 1.7619e-01]],\n",
      "\n",
      "         [[5.9259e-01, 4.4171e-01, 7.0393e-01,  ..., 1.3187e-01,\n",
      "           1.7340e-02, 4.3437e-01],\n",
      "          [7.2956e-01, 6.6240e-01, 9.3666e-01,  ..., 6.6283e-01,\n",
      "           3.6743e-01, 4.2552e-01],\n",
      "          [4.6266e-01, 9.5675e-02, 7.2809e-01,  ..., 3.7576e-01,\n",
      "           9.9000e-01, 6.5350e-01],\n",
      "          ...,\n",
      "          [9.7810e-01, 1.6864e-01, 7.9074e-01,  ..., 1.1427e-01,\n",
      "           1.5436e-01, 9.4154e-01],\n",
      "          [2.7389e-01, 7.2054e-01, 8.7224e-02,  ..., 2.0038e-01,\n",
      "           4.9282e-01, 1.2595e-01],\n",
      "          [7.6776e-01, 7.2812e-01, 7.2489e-01,  ..., 1.6823e-01,\n",
      "           4.3648e-01, 5.4288e-02]],\n",
      "\n",
      "         [[5.3771e-01, 7.5089e-02, 6.4404e-01,  ..., 9.2966e-01,\n",
      "           5.6230e-01, 1.8840e-01],\n",
      "          [5.3401e-01, 5.9596e-01, 3.6474e-01,  ..., 1.1803e-01,\n",
      "           6.7246e-01, 3.6621e-02],\n",
      "          [1.7885e-01, 9.6693e-01, 7.0622e-02,  ..., 9.0225e-01,\n",
      "           2.9191e-02, 4.0709e-01],\n",
      "          ...,\n",
      "          [7.4210e-01, 3.2782e-01, 2.7289e-03,  ..., 3.8752e-01,\n",
      "           1.1112e-01, 2.1763e-01],\n",
      "          [2.0055e-01, 5.9474e-01, 6.6830e-01,  ..., 1.1313e-01,\n",
      "           8.5100e-01, 2.1683e-01],\n",
      "          [3.6461e-01, 7.3131e-01, 7.0599e-01,  ..., 2.6445e-01,\n",
      "           4.5266e-02, 2.7468e-01]]],\n",
      "\n",
      "\n",
      "        [[[6.2003e-01, 6.2864e-01, 2.6872e-01,  ..., 9.1169e-02,\n",
      "           6.3047e-01, 7.7066e-01],\n",
      "          [6.9972e-01, 8.3905e-01, 1.2768e-01,  ..., 2.5151e-01,\n",
      "           4.0225e-02, 2.9207e-01],\n",
      "          [2.7446e-01, 4.1947e-01, 3.4396e-01,  ..., 3.4302e-01,\n",
      "           3.3711e-01, 7.1960e-01],\n",
      "          ...,\n",
      "          [5.1233e-01, 5.3554e-02, 5.2436e-02,  ..., 6.7296e-01,\n",
      "           4.0405e-01, 2.2322e-01],\n",
      "          [3.1220e-02, 2.7115e-01, 2.3005e-01,  ..., 9.0552e-01,\n",
      "           8.8576e-01, 4.1038e-01],\n",
      "          [4.0278e-01, 6.9389e-01, 2.5531e-01,  ..., 9.6135e-01,\n",
      "           8.3683e-01, 5.1307e-01]],\n",
      "\n",
      "         [[9.6848e-01, 2.4931e-01, 6.4020e-01,  ..., 3.2134e-01,\n",
      "           6.5592e-01, 3.8926e-01],\n",
      "          [2.9976e-01, 6.8885e-01, 2.0161e-02,  ..., 3.7163e-01,\n",
      "           2.6026e-01, 2.4702e-01],\n",
      "          [4.8978e-02, 9.2480e-01, 8.1718e-01,  ..., 2.6677e-01,\n",
      "           3.8943e-01, 1.6733e-01],\n",
      "          ...,\n",
      "          [8.7994e-02, 9.5678e-01, 2.3782e-01,  ..., 4.0093e-01,\n",
      "           1.2883e-01, 3.8241e-02],\n",
      "          [5.7315e-01, 5.1292e-01, 9.3284e-01,  ..., 1.9284e-02,\n",
      "           1.4590e-01, 2.7082e-01],\n",
      "          [2.7088e-01, 4.6314e-01, 1.7475e-02,  ..., 7.9576e-01,\n",
      "           7.5836e-01, 3.9696e-01]],\n",
      "\n",
      "         [[9.7474e-01, 9.4902e-02, 1.2024e-01,  ..., 1.4631e-01,\n",
      "           4.6402e-01, 4.7316e-01],\n",
      "          [6.8482e-01, 1.3730e-01, 9.8985e-02,  ..., 1.4936e-01,\n",
      "           3.0250e-01, 9.9941e-01],\n",
      "          [1.8697e-01, 2.7827e-01, 3.7872e-01,  ..., 1.2438e-01,\n",
      "           4.4482e-01, 9.5383e-01],\n",
      "          ...,\n",
      "          [6.9846e-01, 9.3350e-01, 8.1963e-02,  ..., 2.9473e-01,\n",
      "           1.4961e-01, 3.3200e-01],\n",
      "          [5.3043e-01, 3.9626e-01, 4.8936e-01,  ..., 3.3158e-01,\n",
      "           5.7097e-01, 6.2259e-01],\n",
      "          [8.4821e-01, 1.5122e-01, 1.9954e-02,  ..., 9.9368e-01,\n",
      "           3.8803e-01, 8.4261e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.4431e-01, 1.5752e-02, 7.0434e-01,  ..., 8.4940e-01,\n",
      "           4.5391e-02, 4.1823e-01],\n",
      "          [6.8622e-01, 8.9050e-01, 7.5527e-01,  ..., 6.0335e-01,\n",
      "           1.2902e-01, 4.3116e-01],\n",
      "          [8.4372e-01, 7.6210e-01, 5.9299e-01,  ..., 6.6274e-01,\n",
      "           5.5717e-01, 8.2395e-01],\n",
      "          ...,\n",
      "          [1.3437e-01, 4.7454e-01, 3.2003e-01,  ..., 8.9684e-01,\n",
      "           3.4826e-01, 7.0631e-01],\n",
      "          [6.0797e-01, 3.1218e-01, 4.3192e-01,  ..., 1.6145e-01,\n",
      "           7.1306e-01, 7.4228e-01],\n",
      "          [3.1069e-01, 5.7806e-02, 7.2892e-02,  ..., 4.3566e-02,\n",
      "           6.5121e-01, 3.5603e-01]],\n",
      "\n",
      "         [[3.5470e-02, 3.9394e-01, 6.0590e-01,  ..., 8.1658e-01,\n",
      "           1.3322e-01, 1.0181e-01],\n",
      "          [5.7617e-02, 6.9491e-01, 1.4138e-02,  ..., 1.5325e-01,\n",
      "           2.1731e-01, 9.2406e-01],\n",
      "          [2.6805e-01, 9.4842e-01, 1.3381e-01,  ..., 1.3087e-01,\n",
      "           6.8284e-01, 6.5451e-01],\n",
      "          ...,\n",
      "          [7.8883e-01, 7.8634e-01, 1.8353e-01,  ..., 6.2473e-01,\n",
      "           7.9962e-01, 2.3672e-01],\n",
      "          [6.4015e-01, 7.0019e-02, 3.2583e-01,  ..., 3.5477e-01,\n",
      "           4.7128e-01, 1.2187e-02],\n",
      "          [3.9019e-01, 7.0861e-01, 3.7159e-01,  ..., 5.0197e-01,\n",
      "           2.0822e-02, 6.9658e-01]],\n",
      "\n",
      "         [[3.7974e-01, 7.5982e-01, 5.5217e-01,  ..., 3.0749e-01,\n",
      "           1.7727e-01, 9.2318e-01],\n",
      "          [6.5854e-01, 1.7225e-01, 3.5263e-01,  ..., 7.5363e-01,\n",
      "           4.9586e-01, 4.6420e-01],\n",
      "          [8.9807e-01, 1.7317e-01, 2.3032e-01,  ..., 1.8994e-01,\n",
      "           8.9523e-01, 6.8256e-01],\n",
      "          ...,\n",
      "          [5.2279e-01, 7.7495e-01, 6.3207e-02,  ..., 3.9181e-01,\n",
      "           5.1678e-01, 2.3153e-03],\n",
      "          [1.8980e-01, 4.6207e-01, 1.0698e-01,  ..., 4.6820e-03,\n",
      "           7.4276e-02, 1.0314e-01],\n",
      "          [1.3018e-01, 9.7171e-01, 2.4878e-01,  ..., 5.2889e-01,\n",
      "           5.5824e-01, 2.9919e-01]]]])\n",
      "torch.Size([64, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for src_data, tgt_data in test_loader:\n",
    "            output = model(src_data, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, -1)\n",
    "            correct = (predicted == tgt_data[:, 1:]).float()  # Assuming your target is categorical\n",
    "            total_correct += correct.sum().item()\n",
    "            total_samples += tgt_data.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Assuming you have a model, test_loader, and criterion defined\n",
    "test_model(transformer, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

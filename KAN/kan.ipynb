{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employing Kolmogorov-Arnold Networks In Context Of MNIST Dataset:\n",
    "Courtesy of the developers of TorchKan: https://github.com/1ssb/torchkan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Step 1 & 2\n",
    "#Convolutional Feature Extraction: The model begins with two convolutional layers, \n",
    "# each paired with ReLU activation and max-pooling. The first layer employs 16 filters of size 3x3, \n",
    "# while the second increases the feature maps to 32 channels.\n",
    "\n",
    "\n",
    "def feature_extractor():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1,16,kernel_size = 3,stride = 1,padding =1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        nn.Conv2d(16,32,kernel_size = 3,stride = 1,padding =1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Idea:\n",
    "1. Extract the features via Convolutional Feature Extraction\n",
    "\n",
    "2. Apply polynomial feature xform => up to n'th order to flattened convolutional output oin order to discern non-linear relationships\n",
    "\n",
    "3. Monomials (Powers of the polynomials) are calculated to a specific order in order to capture non-linear interactions => richer and informative represenation downstream.\n",
    "\n",
    "4. Monomials calculated are used to adjust the output of the linear layers before activation => introducing an additional dimension of feature intercation => enabling more complex learning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANvolver(nn.Module):\n",
    "    def __init__(self, layers_hidden, poly_order = 2, base_activation = nn.ReLU):\n",
    "        super(KANvolver, self).__init__()\n",
    "        self.layers_hidden = layers_hidden\n",
    "        self.poly_order = poly_order\n",
    "        self.base_activation = base_activation\n",
    "\n",
    "        self.feature_extractor = feature_extractor()\n",
    "\n",
    "        flat_features = 32 * 7 * 7\n",
    "        self.layers_hidden = [flat_features] + self.layers_hidden\n",
    "\n",
    "        self.base_weights = nn.ModuleList()\n",
    "        self.poly_weights = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "\n",
    "        for in_features, out in zip(self.layers_hidden[:-1], self.layers_hidden[1:]):\n",
    "            self.base_weights.append(nn.Linear(in_features, out))\n",
    "            self.poly_weights.append(nn.Linear(in_features * (poly_order + 1), out))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(out))\n",
    "\n",
    "    # How Monomials Work: In the context of this model, monomials are polynomial powers of the input features. \n",
    "    # By computing monomials up to a specified order, the model can capture non-linear interactions between the features, \n",
    "    # potentially leading to richer and more informative representations for downstream tasks.\n",
    "\n",
    "\n",
    "    # They are used to adjust output of the hidden layers before activation => addiitonal dimension of feature interaction => allowing more complex patterns to be learnt\n",
    "\n",
    "    def compute_eff_monomials(self, x, order):\n",
    "        power = torch.arange(order +1, device = x.device, dtype = x.dtype)\n",
    "        x_expanded = x.unsqueeze(-1).repeat(1,1,order+1)\n",
    "        return torch.pow(x_expanded, power)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,28,28)\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # Flatten the features from conv layers\n",
    "\n",
    "        for base, poly, batch_norm in zip(self.base_weights, self.poly_weights, self.batch_norm):\n",
    "            output = base(x)\n",
    "            monomial_base = self.compute_eff_monomials(x, self.poly_order)\n",
    "            monomial_base = monomial_base.view(x.size(0), -1) # Flattening the basis\n",
    "            output_poly = poly(monomial_base)\n",
    "            x = self.base_activation(output + output_poly) # The part where the activation function is applied\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

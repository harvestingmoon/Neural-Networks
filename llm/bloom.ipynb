{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomForCausalLM,BloomTokenizerFast\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pretrained bloom with 1.7b params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I hope to be able to enter into this university\"\n",
    "result_length = 50\n",
    "input = tokenizer(prompt,return_tensors=\"pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we send the model our prompt, we need to think about which decoding / search strategies might work best for our use case. With autoregressive transformers (trained for next token prediction) we have a number of options to search the answer space for the most “reasonable” output. This great article by Patrick von Platen (Huggingface) does an excellent job explaining the details and math behind the 3 techniques we’ll be trying, so I won’t reinvent the wheel here. I will however, give you the TL;DR version of each:\n",
    "\n",
    "Greedy Search simply chooses the next word at each timestep t+1 that has the highest predicted probability of following the word at t. One of the main issues here is that greedy search will miss words with a high probability at t+1 if it is preceded by a word with a low probability at t.\n",
    "\n",
    "\n",
    "Beam Search keeps track of the n-th (num_beams) most likely word sequences and outputs the most likely sequence. Sounds great, but this method breaks down when the output length can be highly variable — as in the case of open-ended text generation. Both greedy and beam search also produce outputs whose distribution does not align very well with the way humans might perform the same task (i.e. both are liable to produce fairly repetitive, boring text).\n",
    "\n",
    "\n",
    "Sampling With Top-k + Top-p is a combination of three methods. By sampling, we mean that the next word is chosen randomly based on its conditional probability distribution (von Platen, 2020). In Top-k, we choose the k most likely words, and then redistribute the probability mass amongst them before the next draw. Top-p adds an additional constraint to top-k, in that we’re choosing from the smallest set of words whose cumulative probability exceed p."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope to be able to enter into this university in the future. I am very grateful to the university for the opportunity to study here. I am very grateful to the university for the opportunity to study here. I am very grateful to the university for\n"
     ]
    }
   ],
   "source": [
    "#greedy search\n",
    "res = model.generate(input[\"input_ids\"],\n",
    "                     max_length=result_length)[0]\n",
    "\n",
    "print(tokenizer.decode(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope to be able to enter into this university in the near future. I am looking forward to learning more about the university and its programs.”\n",
      "“I am very excited to attend the University of Illinois at Urbana-Champaign. The university is a\n"
     ]
    }
   ],
   "source": [
    "#beam search\n",
    "res = model.generate(input[\"input_ids\"],\n",
    "                     max_length=result_length,\n",
    "                     num_beams=2,\n",
    "                     no_repeat_ngram_size=2,\n",
    "                     early_stopping=True)[0]\n",
    "print(tokenizer.decode(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope to be able to enter into this university, please help me. Do you know adversário para enfrentar nesta semifinal?\n",
      "J.C.: Eu espero vencer. Vamos ser bem melhor no segundo tempo, porque é melhor. O jogo está bom,\n"
     ]
    }
   ],
   "source": [
    "#sampling top-k + top-p\n",
    "res = model.generate(input[\"input_ids\"],\n",
    "                     max_length=result_length,\n",
    "                     do_sample=True,\n",
    "                     top_k = 50,\n",
    "                     top_p = 0.9)[0]\n",
    "\n",
    "print(tokenizer.decode(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9363372888fefd0000bb860ba204b24424474f831a16cbd1edd6c13254d10da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
